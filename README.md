# big-data-ingestion
Ingest over 120 daily parquet files since 2016 and create respective delta tables in data lakehouse, with many tables totaling tens of terabytes of data and hundreds of millions of rows. This was done in a Microsoft Azure Databricks environment.
